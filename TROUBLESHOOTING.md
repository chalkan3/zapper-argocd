# Troubleshooting Guide - Zapper ArgoCD

Este documento cont√©m solu√ß√µes para problemas comuns do projeto.

---

## üîç ArgoCD Sync Status "Unknown"

### Problema

```bash
kubectl get applications -n argocd
# NAME              SYNC STATUS   HEALTH STATUS
# clickhouse        Unknown       Healthy
# postgres          Unknown       Healthy
```

### Causa

O status `Unknown` ocorre quando:
1. Recursos foram criados manualmente (`kubectl apply`) antes do ArgoCD
2. ArgoCD n√£o consegue comparar o estado do Git com o cluster
3. Application foi criada ap√≥s os recursos j√° existirem

### Solu√ß√£o 1: Force Sync (Recomendado)

```bash
# For√ßar sync de todas Applications
kubectl get applications -n argocd -o name | xargs -I {} kubectl patch {} -n argocd --type merge -p '{"operation":{"initiatedBy":{"username":"admin"},"sync":{"syncStrategy":{"hook":{}}}}}'

# Ou manualmente para cada app
argocd app sync clickhouse-operator --force
argocd app sync cloudnative-pg-operator --force
argocd app sync peerdb --force
```

### Solu√ß√£o 2: Hard Refresh

```bash
# Hard refresh para recalcular estado
kubectl patch application clickhouse-operator -n argocd \
  --type merge \
  -p '{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}'

# Para todas Applications
kubectl get applications -n argocd -o name | \
  xargs -I {} kubectl patch {} -n argocd \
  --type merge \
  -p '{"metadata":{"annotations":{"argocd.argoproj.io/refresh":"hard"}}}'
```

### Solu√ß√£o 3: Recriar Applications (√öltima Op√ß√£o)

‚ö†Ô∏è **CUIDADO:** Isso vai deletar e recriar as Applications (n√£o os recursos reais)

```bash
# 1. Backup das Applications
kubectl get applications -n argocd -o yaml > argocd-apps-backup.yaml

# 2. Deletar Applications (SEM deletar recursos)
kubectl delete applications -n argocd --all

# 3. Reaplicar do Git
kubectl apply -f apps/

# 4. Aguardar ArgoCD reconciliar
kubectl get applications -n argocd -w
```

### Solu√ß√£o 4: Annotation para Auto-Sync

Adicionar annotation nas Applications para for√ßar sync:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  annotations:
    argocd.argoproj.io/sync-options: Replace=true
    argocd.argoproj.io/compare-options: IgnoreExtraneous
```

Aplicar via patch:

```bash
kubectl patch application clickhouse-operator -n argocd \
  --type merge \
  -p '{"metadata":{"annotations":{"argocd.argoproj.io/sync-options":"Replace=true"}}}'
```

### Verifica√ß√£o

```bash
# Verificar status ap√≥s corre√ß√£o
kubectl get applications -n argocd

# Deve mostrar:
# NAME              SYNC STATUS   HEALTH STATUS
# clickhouse        Synced        Healthy
# postgres          Synced        Healthy
```

---

## üîÑ ArgoCD Sync Loop / OutOfSync Constante

### Problema

Application fica alternando entre `Synced` e `OutOfSync`.

### Causa Comum

1. **Recursos com valores din√¢micos** (timestamps, random IDs)
2. **Defaults aplicados pelo Kubernetes** que n√£o est√£o no Git
3. **Mutating webhooks** modificando recursos

### Solu√ß√£o: Ignorar Diferen√ßas

Editar Application para ignorar campos espec√≠ficos:

```yaml
spec:
  ignoreDifferences:
    - group: apps
      kind: Deployment
      jsonPointers:
        - /spec/replicas  # Ignora se HPA est√° gerenciando

    - group: ""
      kind: Secret
      jsonPointers:
        - /data  # Ignora conte√∫do de secrets

    - group: ""
      kind: Service
      jsonPointers:
        - /spec/clusterIP
        - /spec/clusterIPs
```

Aplicar via patch:

```bash
kubectl patch application peerdb -n argocd --type json \
  -p '[{"op":"add","path":"/spec/ignoreDifferences","value":[{"group":"apps","kind":"Deployment","jsonPointers":["/spec/replicas"]}]}]'
```

---

## üö´ Application Stuck in "Progressing"

### Problema

Application fica em estado `Progressing` por muito tempo.

### Diagn√≥stico

```bash
# Ver detalhes da Application
argocd app get <app-name>

# Ver logs do sync
argocd app logs <app-name>

# Ver recursos criados
kubectl get all -n <namespace>
```

### Causas e Solu√ß√µes

#### 1. Pods n√£o iniciam (ImagePullBackOff)

```bash
# Verificar pods
kubectl get pods -n <namespace>

# Ver eventos
kubectl describe pod <pod-name> -n <namespace>

# Solu√ß√£o: Corrigir image tag ou pull secrets
```

#### 2. Resources Quota excedido

```bash
# Verificar quotas
kubectl describe resourcequota -n <namespace>

# Solu√ß√£o: Aumentar quota ou reduzir requests
```

#### 3. PVC n√£o pode ser provisionado

```bash
# Verificar PVCs
kubectl get pvc -n <namespace>

# Ver eventos
kubectl describe pvc <pvc-name> -n <namespace>

# Solu√ß√£o: Verificar StorageClass existe
kubectl get storageclass
```

---

## üîê Secret "postgres-cluster-app" n√£o encontrado

### Problema

```
Error: secret "postgres-cluster-app" not found
```

### Causa

CloudNativePG espera secret com credenciais, mas ele √© criado automaticamente.

### Solu√ß√£o

Aguardar CloudNativePG Operator criar o secret:

```bash
# Verificar se operator est√° rodando
kubectl get pods -n cloudnative-pg -l app.kubernetes.io/name=cloudnative-pg

# Aguardar secret ser criado
kubectl get secret -n cloudnative-pg -w

# Se n√£o criar, verificar logs do operator
kubectl logs -n cloudnative-pg -l app.kubernetes.io/name=cloudnative-pg
```

---

## üêò PostgreSQL Cluster n√£o inicia

### Diagn√≥stico

```bash
# Verificar cluster
kubectl get cluster -n cloudnative-pg

# Ver pods
kubectl get pods -n cloudnative-pg

# Logs da inst√¢ncia prim√°ria
kubectl logs -n cloudnative-pg postgres-cluster-1
```

### Problemas Comuns

#### 1. Insufficient CPU/Memory

```bash
# Verificar recursos do node
kubectl top nodes

# Solu√ß√£o: Reduzir requests ou adicionar nodes
```

#### 2. Node Affinity n√£o satisfeito

```bash
# Verificar labels dos nodes
kubectl get nodes --show-labels | grep workload

# Solu√ß√£o: Adicionar labels
kubectl label node <node-name> workload=postgres
```

#### 3. Storage Class n√£o existe

```bash
# Verificar storage class
kubectl get storageclass

# Solu√ß√£o: Criar ou usar storageClass: standard
```

---

## üî• ClickHouse Keeper n√£o forma quorum

### Diagn√≥stico

```bash
# Verificar keepers
kubectl get pods -n clickhouse -l clickhouse.altinity.com/keeper

# Logs do keeper
kubectl logs -n clickhouse clickhouse-keeper-0

# Verificar conectividade
kubectl exec -n clickhouse clickhouse-keeper-0 -- \
  echo ruok | nc localhost 9181
```

### Solu√ß√£o

```bash
# Deletar PVCs e reiniciar (dados de teste apenas!)
kubectl delete pvc -n clickhouse -l clickhouse.altinity.com/keeper

# Aguardar keepers subirem novamente
kubectl get pods -n clickhouse -w
```

---

## üîÑ PeerDB n√£o cria mirror

### Diagn√≥stico

```bash
# Verificar job de setup
kubectl get jobs -n peerdb

# Logs do job
kubectl logs -n peerdb job/peerdb-setup-mirror

# Verificar PeerDB est√° acess√≠vel
kubectl port-forward -n peerdb svc/peerdb 3000:3000
curl http://localhost:3000/api/health
```

### Problemas Comuns

#### 1. PostgreSQL n√£o est√° pronto

```bash
# Aguardar PostgreSQL cluster
kubectl wait --for=condition=ready cluster/postgres-cluster \
  -n cloudnative-pg --timeout=600s
```

#### 2. ClickHouse n√£o est√° pronto

```bash
# Verificar ClickHouse pods
kubectl get pods -n clickhouse

# Testar conex√£o
kubectl exec -n clickhouse chi-clickhouse-cluster-clickhouse-0-0-0 -- \
  clickhouse-client --query "SELECT 1"
```

#### 3. Temporal n√£o est√° pronto

```bash
# Verificar Temporal
kubectl get pods -n peerdb -l app.kubernetes.io/name=temporal

# Logs
kubectl logs -n peerdb -l app.kubernetes.io/name=temporal
```

### Solu√ß√£o: Reexecutar Job

```bash
# Deletar job antigo
kubectl delete job -n peerdb peerdb-setup-mirror

# Recriar job
kubectl apply -f manifests/peerdb/setup-mirror-job.yaml
```

---

## üìä Prometheus n√£o scrape m√©tricas

### Diagn√≥stico

```bash
# Verificar ServiceMonitors
kubectl get servicemonitor -n monitoring

# Verificar targets no Prometheus
kubectl port-forward -n monitoring svc/kube-prometheus-stack-prometheus 9090:9090
# Acessar: http://localhost:9090/targets
```

### Solu√ß√£o

```bash
# Verificar se Prometheus Operator est√° rodando
kubectl get pods -n monitoring -l app.kubernetes.io/name=prometheus-operator

# Verificar labels dos services
kubectl get svc -n clickhouse --show-labels
kubectl get svc -n cloudnative-pg --show-labels

# Aplicar ServiceMonitors novamente
kubectl apply -f manifests/monitoring/
```

---

## üîç Comandos √öteis de Debug

### ArgoCD

```bash
# Ver todas applications
argocd app list

# Detalhes de uma app
argocd app get <app-name>

# Diff entre Git e cluster
argocd app diff <app-name>

# Force sync
argocd app sync <app-name> --force

# Deletar app (mant√©m recursos)
argocd app delete <app-name> --cascade=false
```

### Kubernetes

```bash
# Ver todos recursos em namespace
kubectl get all -n <namespace>

# Eventos (√∫ltimos problemas)
kubectl get events -n <namespace> --sort-by='.lastTimestamp'

# Logs de todos pods de um deployment
kubectl logs -n <namespace> -l app=<label> --tail=100

# Describe (detalhes + eventos)
kubectl describe <resource> <name> -n <namespace>

# Port forward para debug
kubectl port-forward -n <namespace> <pod-name> <local-port>:<remote-port>
```

### PostgreSQL

```bash
# Conectar ao PostgreSQL
kubectl exec -it -n cloudnative-pg postgres-cluster-1 -- \
  psql -U app_user -d app_db

# Verificar replica√ß√£o
kubectl exec -n cloudnative-pg postgres-cluster-1 -- \
  psql -U postgres -c "SELECT * FROM pg_stat_replication;"

# Verificar WAL
kubectl exec -n cloudnative-pg postgres-cluster-1 -- \
  psql -U postgres -c "SELECT pg_current_wal_lsn();"
```

### ClickHouse

```bash
# Conectar ao ClickHouse
kubectl exec -it -n clickhouse chi-clickhouse-cluster-clickhouse-0-0-0 -- \
  clickhouse-client

# Verificar cluster
kubectl exec -n clickhouse chi-clickhouse-cluster-clickhouse-0-0-0 -- \
  clickhouse-client --query "SELECT * FROM system.clusters"

# Verificar replica√ß√£o
kubectl exec -n clickhouse chi-clickhouse-cluster-clickhouse-0-0-0 -- \
  clickhouse-client --query "SELECT * FROM system.replicas"
```

### PeerDB

```bash
# Conectar ao PeerDB
kubectl port-forward -n peerdb svc/peerdb 3000:3000

# Verificar peers
curl http://localhost:3000/api/v1/peers

# Verificar mirrors
curl http://localhost:3000/api/v1/mirrors
```

---

## üö® Recovery Procedures

### Reinstala√ß√£o Completa

```bash
# 1. Backup (se necess√°rio)
kubectl get all --all-namespaces -o yaml > backup.yaml

# 2. Deletar tudo
make cleanup  # Ou usar scripts/cleanup.sh

# 3. Reinstalar
make install-argocd
kubectl apply -f apps/

# 4. Aguardar
kubectl get applications -n argocd -w
```

### Recovery de Dados PostgreSQL

```bash
# Se configurou backup S3
kubectl exec -n cloudnative-pg postgres-cluster-1 -- \
  barman-cloud-restore <s3-path> <backup-id> /var/lib/postgresql/data
```

### Recovery de ClickHouse

```bash
# ClickHouse mant√©m dados em PVCs
# Para restaurar, basta recriar pods com mesmos PVCs
kubectl delete pod -n clickhouse <pod-name>
# PVC mant√©m dados, novo pod monta mesmo volume
```

---

## üìû Obter Ajuda

### Logs importantes

```bash
# Coletar logs para an√°lise
kubectl logs -n argocd -l app.kubernetes.io/name=argocd-server > argocd.log
kubectl logs -n cloudnative-pg -l app.kubernetes.io/name=cloudnative-pg > cnpg.log
kubectl logs -n clickhouse -l clickhouse.altinity.com/app=chop > clickhouse-operator.log
kubectl get events --all-namespaces --sort-by='.lastTimestamp' > events.log
```

### Informa√ß√µes do cluster

```bash
kubectl version
kubectl get nodes -o wide
kubectl top nodes
kubectl get storageclass
kubectl get all --all-namespaces
```

---

**√öltima atualiza√ß√£o:** Outubro 2024
**Vers√£o:** 1.0
