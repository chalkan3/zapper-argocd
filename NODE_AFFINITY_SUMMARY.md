# ğŸ¯ Node Affinity & HPA - Resumo Executivo

## âœ… ConfiguraÃ§Ã£o Completa

Todo o repositÃ³rio foi configurado para:
1. **Node Affinity**: Pods deployados em workers especÃ­ficos
2. **HPA**: Auto-scaling baseado em CPU e memÃ³ria

---

## ğŸ“Š DistribuiÃ§Ã£o de Workers (5 nodes)

```
Worker 1-2  â†’ PostgreSQL      (label: workload=postgres)
Worker 3    â†’ ClickHouse      (label: workload=clickhouse)
Worker 4-5  â†’ PeerDB          (label: workload=peerdb)
```

### Tabela de DistribuiÃ§Ã£o

| Workers | Label | Pods | Componentes |
|---------|-------|------|-------------|
| 1-2 | `postgres` | 3 | CloudNativePG (1 primary + 2 standby) |
| 3 | `clickhouse` | 7 | ClickHouse (4 pods) + Keeper (3 pods) |
| 4-5 | `peerdb` | 8+ | PeerDB + PostgreSQL + Temporal |

**Total**: ~18+ pods distribuÃ­dos em 5 workers

---

## ğŸš€ Quick Start

### 1. Adicionar Labels aos Workers

```bash
# PostgreSQL workers
kubectl label node worker-1 workload=postgres
kubectl label node worker-2 workload=postgres

# ClickHouse worker
kubectl label node worker-3 workload=clickhouse

# PeerDB workers
kubectl label node worker-4 workload=peerdb
kubectl label node worker-5 workload=peerdb
```

### 2. Adicionar Taints (Opcional)

```bash
kubectl taint node worker-1 workload=postgres:NoSchedule
kubectl taint node worker-2 workload=postgres:NoSchedule
kubectl taint node worker-3 workload=clickhouse:NoSchedule
kubectl taint node worker-4 workload=peerdb:NoSchedule
kubectl taint node worker-5 workload=peerdb:NoSchedule
```

### 3. Deploy

```bash
kubectl apply -f apps/
```

### 4. Verificar

```bash
# Ver distribuiÃ§Ã£o de pods
kubectl get pods -o wide --all-namespaces | grep -E "clickhouse|postgres|peerdb"

# Ver HPAs
kubectl get hpa --all-namespaces
```

---

## ğŸ“ˆ HPAs Configurados (9 HPAs)

| HPA | Min | Max | CPU | Memory |
|-----|-----|-----|-----|--------|
| PeerDB Server | 1 | 5 | 70% | 80% |
| PeerDB Flow Workers | 2 | 10 | 75% | 80% |
| CloudNativePG | 3 | 5 | 70% | 80% |
| PeerDB PostgreSQL | 1 | 3 | 75% | 80% |
| ClickHouse | 4 | 8 | 70% | 75% |
| Temporal Frontend | 1 | 5 | 70% | 80% |
| Temporal History | 1 | 5 | 70% | 80% |
| Temporal Matching | 1 | 5 | 70% | 80% |
| Temporal Worker | 1 | 10 | 75% | 80% |

---

## ğŸ“ Arquivos Modificados

### Node Affinity Adicionado

âœ… `helm-values/postgres-cluster.yaml` - CloudNativePG
âœ… `helm-values/clickhouse-cluster.yaml` - ClickHouse + Keeper
âœ… `manifests/peerdb/deployment.yaml` - PeerDB Server + Workers
âœ… `apps/peerdb-dependencies.yaml` - PostgreSQL + Temporal

### HPAs Criados

âœ… `manifests/hpa/peerdb-hpa.yaml`
âœ… `manifests/hpa/postgres-hpa.yaml`
âœ… `manifests/hpa/clickhouse-hpa.yaml`
âœ… `manifests/hpa/temporal-hpa.yaml`

### Application Adicionada

âœ… `apps/hpa.yaml` - ArgoCD Application para HPAs

---

## ğŸ”§ PrÃ©-requisitos

### Metrics Server

HPAs requerem o Metrics Server:

```bash
# Verificar
kubectl get deployment metrics-server -n kube-system

# Instalar (se necessÃ¡rio)
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

**Nota**: K3s jÃ¡ inclui metrics-server por padrÃ£o.

---

## ğŸ“Š Exemplo de DistribuiÃ§Ã£o Final

```
worker-1 (postgres):
  â”œâ”€â”€ postgres-cluster-1 (primary)
  â””â”€â”€ postgres-cluster-3 (standby)

worker-2 (postgres):
  â””â”€â”€ postgres-cluster-2 (standby)

worker-3 (clickhouse):
  â”œâ”€â”€ clickhouse-0-0-0 (shard 1, replica 1)
  â”œâ”€â”€ clickhouse-0-1-0 (shard 1, replica 2)
  â”œâ”€â”€ clickhouse-1-0-0 (shard 2, replica 1)
  â”œâ”€â”€ clickhouse-1-1-0 (shard 2, replica 2)
  â”œâ”€â”€ clickhouse-keeper-0
  â”œâ”€â”€ clickhouse-keeper-1
  â””â”€â”€ clickhouse-keeper-2

worker-4 (peerdb):
  â”œâ”€â”€ peerdb-server-xxx
  â”œâ”€â”€ peerdb-postgresql-0
  â”œâ”€â”€ peerdb-temporal-frontend-xxx
  â””â”€â”€ peerdb-temporal-history-xxx

worker-5 (peerdb):
  â”œâ”€â”€ peerdb-flow-worker-xxx-1
  â”œâ”€â”€ peerdb-flow-worker-xxx-2
  â”œâ”€â”€ peerdb-temporal-matching-xxx
  â””â”€â”€ peerdb-temporal-worker-xxx
```

---

## ğŸ¯ Total de Applications

Agora sÃ£o **8 Applications** (era 7):

1. clickhouse-operator
2. clickhouse-cluster
3. cloudnative-pg-operator
4. postgres-cluster
5. peerdb-postgresql
6. peerdb-temporal
7. peerdb
8. **hpa** â† NOVO

---

## ğŸ“š DocumentaÃ§Ã£o Completa

â†’ **NODE_AFFINITY_SETUP.md** - Guia completo com troubleshooting

---

## âœ… Checklist RÃ¡pido

- [ ] Aplicar labels nos 5 workers
- [ ] (Opcional) Aplicar taints nos workers
- [ ] Deploy: `kubectl apply -f apps/`
- [ ] Verificar distribuiÃ§Ã£o: `kubectl get pods -o wide --all-namespaces`
- [ ] Verificar HPAs: `kubectl get hpa --all-namespaces`
- [ ] Monitorar: `kubectl top pods -n peerdb`

---

**Status**: âœ… **COMPLETO** - Node Affinity + HPA configurados
