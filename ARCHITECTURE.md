# Arquitetura e Racional do Projeto

## ğŸ“‹ SumÃ¡rio Executivo

Este documento explica as decisÃµes arquiteturais, tecnologias escolhidas e racional por trÃ¡s da implementaÃ§Ã£o da infraestrutura de dados distribuÃ­da com CDC (Change Data Capture) usando GitOps.

---

## ğŸ¯ Objetivos do Projeto

### Requisitos Principais

1. **ClickHouse em Cluster Mode**
   - ConfiguraÃ§Ã£o com Keeper para coordenaÃ§Ã£o distribuÃ­da
   - DistribuiÃ§Ã£o de dados em 2+ nodes via sharding
   - Alta disponibilidade com replicaÃ§Ã£o

2. **PostgreSQL com CloudNativePG**
   - InstÃ¢ncia com dados de teste (dummy data)
   - Preparada para replicaÃ§Ã£o via CDC

3. **CDC com PeerDB**
   - ETL em tempo real PostgreSQL â†’ ClickHouse
   - ConfiguraÃ§Ã£o de peers (sources) e mirrors
   - Garantia de rÃ©plica contÃ­nua dos dados

4. **DependÃªncias do PeerDB**
   - PostgreSQL para metadata do PeerDB
   - Temporal para orquestraÃ§Ã£o de workflows

5. **GitOps Completo**
   - Infraestrutura como cÃ³digo
   - Versionamento e auditoria
   - Deploy automatizado via ArgoCD

---

## ğŸ—ï¸ Arquitetura Geral

### Diagrama de Componentes

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Git Repository (Source of Truth)             â”‚
â”‚  â”œâ”€ apps/                    (ArgoCD Applications)              â”‚
â”‚  â”œâ”€ manifests/               (Kubernetes Manifests)             â”‚
â”‚  â”œâ”€ helm-values/             (Helm Values)                      â”‚
â”‚  â””â”€ scripts/                 (Automation Scripts)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ArgoCD                                  â”‚
â”‚  - Sincroniza Git â†’ Kubernetes                                  â”‚
â”‚  - Sync Waves (ordem de deploy)                                 â”‚
â”‚  - Auto-healing e self-healing                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â–¼              â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ClickHouse    â”‚ â”‚ PostgreSQL â”‚ â”‚     PeerDB       â”‚
â”‚   Operator     â”‚ â”‚  Operator  â”‚ â”‚   + Temporal     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                â”‚              â”‚
        â–¼                â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ClickHouse     â”‚ â”‚PostgreSQL  â”‚ â”‚ PeerDB Metadata  â”‚
â”‚ Cluster        â”‚ â”‚ Cluster    â”‚ â”‚ PostgreSQL       â”‚
â”‚                â”‚ â”‚            â”‚ â”‚                  â”‚
â”‚ - 2 Shards     â”‚ â”‚- 3 Replicasâ”‚ â”‚ Temporal Server  â”‚
â”‚ - 2 Replicas   â”‚ â”‚- WAL CDC   â”‚ â”‚                  â”‚
â”‚ - 3 Keepers    â”‚ â”‚- Dummy Dataâ”‚ â”‚ PeerDB Server    â”‚
â”‚                â”‚ â”‚            â”‚ â”‚ + Workers        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â–²                â”‚                    â”‚
        â”‚                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    CDC Replication
        â”‚                   (PG â†’ ClickHouse)
        â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Prometheus + Grafana (Monitoring)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¤” DecisÃµes Arquiteturais e Racional

### 1. Por que ArgoCD (GitOps)?

**DecisÃ£o:** Usar ArgoCD como ferramenta de GitOps.

**Racional:**
- âœ… **Declarativo**: Infraestrutura versionada no Git
- âœ… **AuditÃ¡vel**: HistÃ³rico completo de mudanÃ§as
- âœ… **RecuperaÃ§Ã£o**: Rollback fÃ¡cil via Git
- âœ… **Sync Waves**: Controle preciso da ordem de deploy
- âœ… **Self-healing**: CorreÃ§Ã£o automÃ¡tica de drift
- âœ… **UI + CLI**: Interface visual + automaÃ§Ã£o

**Alternativas consideradas:**
- Flux CD: Menos maduro, UI limitada
- Manual kubectl: NÃ£o escalÃ¡vel, sem histÃ³rico
- Helm direto: Sem GitOps, sem auditoria

---

### 2. Por que ClickHouse Operator?

**DecisÃ£o:** Usar Altinity ClickHouse Operator com Keeper.

**Racional:**
- âœ… **Cluster Nativo**: Suporte a sharding e replicaÃ§Ã£o
- âœ… **ClickHouse Keeper**: Substituto do ZooKeeper (mais leve)
- âœ… **Operador Maduro**: Altinity Ã© mantido pela comunidade ClickHouse
- âœ… **CRDs Declarativas**: `ClickHouseInstallation` + `ClickHouseKeeperInstallation`
- âœ… **Auto-scaling**: Suporte a HPAs

**ConfiguraÃ§Ã£o escolhida:**
```yaml
shardsCount: 2        # DistribuiÃ§Ã£o de dados em 2 shards
replicasCount: 2      # Alta disponibilidade com 2 rÃ©plicas
keeperReplicas: 3     # Quorum de 3 Keepers (HA)
```

**Por que 2 shards?**
- Balanceamento de carga de queries
- ParalelizaÃ§Ã£o de ingestÃ£o
- Escalabilidade horizontal

**Por que 3 Keepers?**
- Quorum para consenso (tolerÃ¢ncia a 1 falha)
- CoordenaÃ§Ã£o de replicaÃ§Ã£o distribuÃ­da
- Leve comparado ao ZooKeeper

---

### 3. Por que CloudNativePG?

**DecisÃ£o:** Usar CloudNativePG como operador PostgreSQL.

**Racional:**
- âœ… **Cloud Native**: Projetado para Kubernetes
- âœ… **HA Nativo**: ReplicaÃ§Ã£o streaming automÃ¡tica
- âœ… **CDC Ready**: WAL level = logical (suporte a CDC)
- âœ… **Backup/Recovery**: IntegraÃ§Ã£o com S3, Azure, GCS
- âœ… **Pooling**: PgBouncer integrado
- âœ… **Open Source**: CNCF Sandbox Project

**ConfiguraÃ§Ã£o CDC:**
```yaml
wal_level: logical              # NecessÃ¡rio para CDC
max_wal_senders: 10             # ConexÃµes de replicaÃ§Ã£o
max_replication_slots: 10       # Slots para PeerDB
REPLICA IDENTITY FULL           # Captura completa de mudanÃ§as
```

**Alternativas consideradas:**
- Zalando Postgres Operator: Mais complexo, menos maduro
- Bitnami PostgreSQL HA: Sem operador, configuraÃ§Ã£o manual
- CrunchyData: Comercial, features bloqueadas

---

### 4. Por que PeerDB?

**DecisÃ£o:** Usar PeerDB para CDC/ETL PostgreSQL â†’ ClickHouse.

**Racional:**
- âœ… **CDC Nativo**: Projetado especificamente para PG â†’ CH
- âœ… **Desempenho**: 10x mais rÃ¡pido que Airbyte/Debezium
- âœ… **Tipo-Safe**: Mapeamento automÃ¡tico de tipos PG â†” CH
- âœ… **Initial Snapshot**: Carga inicial + CDC contÃ­nuo
- âœ… **UI Simples**: Interface para configurar mirrors
- âœ… **API REST**: AutomaÃ§Ã£o via scripts

**Fluxo CDC:**
```
PostgreSQL WAL â†’ PeerDB Workers â†’ ClickHouse Tables
     â†“                â†“                    â†“
(Logical Decoding) (Processing)     (Batch Insert)
```

**Alternativas consideradas:**
- Debezium + Kafka: Muito complexo, overhead desnecessÃ¡rio
- Airbyte: Lento, nÃ£o otimizado para PG â†’ CH
- Scripts custom: NÃ£o confiÃ¡vel, manutenÃ§Ã£o alta

---

### 5. Por que Temporal?

**DecisÃ£o:** Instalar Temporal como dependÃªncia do PeerDB.

**Racional:**
- âœ… **Requisito do PeerDB**: PeerDB usa Temporal para workflows
- âœ… **OrquestraÃ§Ã£o**: Coordena workers de CDC
- âœ… **Retry Logic**: ResiliÃªncia automÃ¡tica
- âœ… **Observabilidade**: UI para debug de workflows
- âœ… **Durabilidade**: Workflows sobrevivem a restarts

**Como funciona:**
```
PeerDB â†’ Temporal Workflow â†’ CDC Workers
   â”‚           â”‚                   â”‚
   â”‚           â–¼                   â–¼
   â”‚     (State Machine)    (Process Changes)
   â”‚           â”‚                   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         (Metadata PostgreSQL)
```

---

### 6. EstratÃ©gia de Deploy com Sync Waves

**DecisÃ£o:** Usar sync waves + prefixos numÃ©ricos nos arquivos.

**Racional:**
- âœ… **Ordem Garantida**: Operators antes de clusters
- âœ… **DependÃªncias**: PeerDB sÃ³ apÃ³s PG + CH prontos
- âœ… **TolerÃ¢ncia a Falhas**: Retry automÃ¡tico por fase
- âœ… **Pulumi-safe**: Prefixos garantem ordem alfabÃ©tica

**Fases de Deploy:**
```
Wave 1 (01-02): Operators
  â”œâ”€ ClickHouse Operator
  â””â”€ CloudNativePG Operator

Wave 2 (03): Clusters + Dependencies
  â”œâ”€ ClickHouse Cluster (com Keeper)
  â”œâ”€ PostgreSQL Cluster
  â”œâ”€ PeerDB PostgreSQL
  â””â”€ Temporal

Wave 3 (04-06): Applications
  â”œâ”€ PeerDB Server
  â”œâ”€ HPAs
  â””â”€ Monitoring Stack

Wave 4 (07): Setup Jobs
  â”œâ”€ Seed Data Job
  â””â”€ PeerDB CDC Mirror Setup
```

---

### 7. Node Affinity e Tolerations

**DecisÃ£o:** Distribuir workloads em workers dedicados.

**Racional:**
- âœ… **Isolamento**: Evita noisy neighbors
- âœ… **Performance**: Recursos dedicados por workload
- âœ… **Escalabilidade**: Adicionar nodes por tipo
- âœ… **Custos**: Otimizar instÃ¢ncias por workload

**DistribuiÃ§Ã£o:**
```
worker-1, worker-2 â†’ PostgreSQL (I/O intensivo)
worker-3           â†’ ClickHouse (CPU/MemÃ³ria)
worker-4, worker-5 â†’ PeerDB (CPU para CDC)
```

**ImplementaÃ§Ã£o:**
```yaml
affinity:
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
            - key: workload
              operator: In
              values:
                - postgres  # ou clickhouse, peerdb
```

---

### 8. Auto-scaling (HPAs)

**DecisÃ£o:** Configurar HPAs para todos os componentes.

**Racional:**
- âœ… **Elasticidade**: Escala com carga real
- âœ… **Custos**: Reduz pods em perÃ­odos ociosos
- âœ… **SLA**: MantÃ©m performance sob carga
- âœ… **AutomÃ¡tico**: Sem intervenÃ§Ã£o manual

**HPAs configurados:**
```
- PostgreSQL: 3-10 replicas (CPU 70%)
- ClickHouse: 4-12 pods (CPU 75%)
- PeerDB Server: 1-5 replicas (CPU 70%)
- PeerDB Workers: 2-8 replicas (CPU 80%)
- Temporal: 1-3 replicas (CPU 70%)
```

---

### 9. Monitoring com Prometheus + Grafana

**DecisÃ£o:** Deploy completo do kube-prometheus-stack.

**Racional:**
- âœ… **Observabilidade**: MÃ©tricas de todos componentes
- âœ… **Alertas**: Alertmanager para incidentes
- âœ… **Dashboards**: VisualizaÃ§Ã£o em tempo real
- âœ… **Service Discovery**: Auto-discovery de targets
- âœ… **Long-term Storage**: RetenÃ§Ã£o de 30 dias

**Dashboards incluÃ­dos:**
- Kubernetes Cluster (gnetId: 7249)
- ClickHouse Overview (gnetId: 882)
- PostgreSQL Database (gnetId: 9628)
- ArgoCD (gnetId: 14584)

---

### 10. AutomaÃ§Ã£o de CDC Mirror

**DecisÃ£o:** Script Python + Kubernetes Job para setup automÃ¡tico.

**Racional:**
- âœ… **Zero Touch**: CDC configurado automaticamente
- âœ… **Idempotente**: Pode reexecutar sem erros
- âœ… **ValidaÃ§Ã£o**: Verifica conectividade antes de criar
- âœ… **GitOps**: Job versionado no Git

**Fluxo:**
```python
1. Aguarda PeerDB estar pronto
2. LÃª senha do PostgreSQL (Kubernetes Secret)
3. Cria peer PostgreSQL (postgres-source)
4. Cria peer ClickHouse (clickhouse-destination)
5. Cria mirror PGâ†’CH (tables: users, orders, events)
6. Valida replicaÃ§Ã£o inicial
```

---

## ğŸ“Š EspecificaÃ§Ãµes TÃ©cnicas

### Recursos Computacionais

| Componente | CPU Request | Memory Request | CPU Limit | Memory Limit |
|------------|-------------|----------------|-----------|--------------|
| PostgreSQL | 500m | 1Gi | 2000m | 4Gi |
| ClickHouse | 1000m | 2Gi | 4000m | 8Gi |
| PeerDB Server | 500m | 512Mi | 1000m | 2Gi |
| Temporal | 500m | 512Mi | 1000m | 1Gi |
| Prometheus | 500m | 2Gi | 2000m | 8Gi |
| Grafana | 100m | 256Mi | 500m | 1Gi |

### Storage

| Componente | Tipo | Tamanho | RetenÃ§Ã£o |
|------------|------|---------|----------|
| PostgreSQL Data | RWO | 20Gi | - |
| ClickHouse Data | RWO | 10Gi/pod | - |
| ClickHouse Logs | RWO | 5Gi/pod | - |
| Keeper Data | RWO | 5Gi/pod | - |
| Prometheus | RWO | 50Gi | 30d |
| Grafana | RWO | 10Gi | - |

### Network Policies

```
argocd       â†’ Todos (gerenciamento)
peerdb       â†’ postgres-cluster (CDC source)
peerdb       â†’ clickhouse-cluster (CDC destination)
prometheus   â†’ Todos (scraping)
grafana      â†’ prometheus (datasource)
```

---

## ğŸ”„ Fluxo de Dados (CDC)

### 1. Escrita no PostgreSQL
```sql
INSERT INTO users (username, email) VALUES ('alice', 'alice@example.com');
```

### 2. Captura no WAL (Write-Ahead Log)
```
PostgreSQL WAL â†’ Logical Decoding â†’ Replication Slot
```

### 3. Processamento PeerDB
```
PeerDB Worker â†’ LÃª WAL changes â†’ Transforma tipos â†’ Batch
```

### 4. Escrita no ClickHouse
```
ClickHouse â†’ INSERT INTO users (username, email) VALUES (...)
```

### 5. VerificaÃ§Ã£o
```sql
-- PostgreSQL
SELECT COUNT(*) FROM users; -- 100

-- ClickHouse (apÃ³s ~5s)
SELECT COUNT(*) FROM users; -- 100 âœ…
```

---

## ğŸ§ª ValidaÃ§Ã£o e Testes

### Testes Automatizados (scripts/test-e2e.sh)

1. **Connectivity Tests**
   - âœ… PostgreSQL cluster acessÃ­vel
   - âœ… ClickHouse cluster acessÃ­vel
   - âœ… PeerDB API respondendo

2. **Data Validation**
   - âœ… PostgreSQL tem dados (users, orders, events)
   - âœ… ClickHouse recebeu dados via CDC
   - âœ… Contagem de registros confere

3. **CDC Health**
   - âœ… Mirror ativo e sincronizando
   - âœ… Replication lag < 10s
   - âœ… Sem erros nos workers

4. **Monitoring**
   - âœ… Prometheus scraping targets
   - âœ… Grafana dashboards carregados
   - âœ… Alertmanager configurado

---

## ğŸš€ Deployment Pipeline

### Ordem de ExecuÃ§Ã£o

```
1. Git Push â†’ main branch
   â†“
2. ArgoCD detecta mudanÃ§a (3min sync)
   â†“
3. Sync Wave 1: Operators instalados
   â”œâ”€ ClickHouse Operator
   â””â”€ CloudNativePG Operator
   â†“
4. Sync Wave 2: Clusters criados
   â”œâ”€ ClickHouse Cluster (2 shards, 2 replicas, 3 keepers)
   â”œâ”€ PostgreSQL Cluster (3 instÃ¢ncias)
   â”œâ”€ PeerDB PostgreSQL
   â””â”€ Temporal
   â†“
5. Sync Wave 3: Applications
   â”œâ”€ PeerDB Server + Workers
   â”œâ”€ HPAs
   â””â”€ Prometheus + Grafana
   â†“
6. Sync Wave 4: Setup Jobs
   â”œâ”€ Seed Data Job (popular PostgreSQL)
   â””â”€ PeerDB Setup Job (criar CDC mirror)
   â†“
7. âœ… Stack completo rodando
   â”œâ”€ PostgreSQL replicando â†’ ClickHouse
   â”œâ”€ Monitoring ativo
   â””â”€ Auto-scaling configurado
```

**Tempo total:** ~10-15 minutos

---

## ğŸ¯ Objetivos Cumpridos

| Requisito | Status | EvidÃªncia |
|-----------|--------|-----------|
| ClickHouse cluster mode com Keeper | âœ… | `helm-values/clickhouse-cluster.yaml:31-32` |
| Sharding em 2+ nodes | âœ… | `shardsCount: 2` |
| CloudNativePG com dummy data | âœ… | `manifests/cloudnative-pg/seed-data-job.yaml` |
| CDC PGâ†’CH via PeerDB | âœ… | `scripts/setup-peerdb-mirror.py` |
| PostgreSQL + Temporal (deps) | âœ… | `apps/03-peerdb-dependencies.yaml` |
| RÃ©plica contÃ­nua validada | âœ… | `scripts/test-e2e.sh` |
| Repo com source-code | âœ… | GitHub repo completo |
| Outline/racional | âœ… | Este documento |
| InstruÃ§Ãµes de execuÃ§Ã£o | âœ… | `README.md` + `CONTRIBUTING.md` |

---

## ğŸ“š ReferÃªncias TÃ©cnicas

- [ClickHouse Operator](https://github.com/Altinity/clickhouse-operator)
- [CloudNativePG](https://cloudnative-pg.io/)
- [PeerDB](https://docs.peerdb.io/)
- [Temporal](https://temporal.io/)
- [ArgoCD](https://argo-cd.readthedocs.io/)
- [Prometheus Operator](https://prometheus-operator.dev/)

---

## ğŸ”® EvoluÃ§Ã£o Futura

### Melhorias Planejadas

1. **Multi-tenancy**: Isolamento por namespace
2. **Backup/DR**: Velero + Restic
3. **Security**: Network Policies + OPA
4. **CI/CD**: GitHub Actions + Validation
5. **Observability**: Jaeger + OpenTelemetry
6. **Escalabilidade**: Adicionar mais shards ClickHouse

---

**Autor:** Chalkan3
**Data:** Outubro 2024
**VersÃ£o:** 1.0
